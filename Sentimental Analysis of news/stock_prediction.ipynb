{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA,FastICA\n",
    "import operator\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import datetime as dt \n",
    "import time\n",
    "\n",
    "# from wordCount_samsung import *\n",
    "\n",
    "def combine_final_df(symbol):\n",
    "    '''\n",
    "    df: Fundamental Info + Yahoo Daily Price + Sentiments\n",
    "    '''\n",
    "    # Processing Fundamental Info\n",
    "    print(symbol)\n",
    "    guru = 'GuruFocus/' + symbol + '_Guru.csv'\n",
    "    fundamental = pd.read_csv(guru, encoding = \"ISO-8859-1\")\n",
    "    fundamental['Filing Date'] = pd.to_datetime(fundamental['Filing Date'])\n",
    "\n",
    "    idx = pd.date_range('2011-01-01', '2017-07-15')\n",
    "    fundamental.set_index('Filing Date', inplace=True)\n",
    "    fundamental.index= pd.DatetimeIndex(fundamental.index)\n",
    "    fundamental = fundamental.reindex(idx, method='bfill')\n",
    "\n",
    "    # Sentiment Scores\n",
    "    sentiment = 'SentimentNews/Abnormal_' + symbol +'_News.csv'\n",
    "    news_df = pd.read_csv(sentiment, encoding = \"ISO-8859-1\")\n",
    "    news_df = news_df[news_df.Text != \"['nannan']\" ]\n",
    "\n",
    "    data= news_df.values\n",
    "    document = []\n",
    "    for row in data:\n",
    "        score = row[2].split(',')\n",
    "        if score[0][2:5] == 'pos':\n",
    "            document.append( [row[0], row[1], float(score[1][1:-1]) ])\n",
    "        elif score[0][2:5] == 'neg':\n",
    "            document.append( [row[0], row[1], float(score[1][1:-1])* -1.0 ])\n",
    "\n",
    "\n",
    "    news_df = pd.DataFrame(document, columns=['Date','Text','Score'])\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "    del news_df['Text']\n",
    "\n",
    "    # Stock Price df & Generate Labels\n",
    "    price = 'News/' + symbol+'_Stocks.csv'\n",
    "    price_df = pd.read_csv(price, encoding = \"ISO-8859-1\")\n",
    "    price_df['Return'] = price_df['Adj Close'].pct_change()\n",
    "\n",
    "    sp500 = pd.read_csv('GuruFocus/Yahoo_Index_GSPC.csv')\n",
    "    sp500['Date'] = pd.to_datetime(sp500['Date'])\n",
    "    sp500['Sp_Return'] = sp500['Adj Close'].pct_change()\n",
    "    del sp500['Open']\n",
    "    del sp500['Close']\n",
    "    del sp500['High']\n",
    "    del sp500['Low']\n",
    "    del sp500['Volume']\n",
    "    del sp500['Adj Close']\n",
    "\n",
    "    price_df= price_df.set_index('Date').join(sp500.set_index('Date'))\n",
    "    price_df = price_df.dropna()\n",
    "    price_df['Difference'] = price_df['Return'] - price_df['Sp_Return']\n",
    "\n",
    "    # Stock Price + Fundamental Info + Sentiments\n",
    "    sentiment_stock = news_df.set_index('Date').join(price_df)\n",
    "    df = sentiment_stock.join(fundamental)\n",
    "    df = df[pd.notnull(df.index)]\n",
    "    df = df.dropna()\n",
    "    del df['Fiscal Period']\n",
    "    del df['Restated Filing Date']\n",
    "    df = df.replace(to_replace = ['No Debt'], value = 0)\n",
    "\n",
    "    df['Target'] = np.nan\n",
    "    requirement = 0.0\n",
    "    for i in range(len(df)):\n",
    "        if df['Difference'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Difference'].iloc[i] <  requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "    del df['Sp_Return']\n",
    "\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    if 'index' in df.columns:\n",
    "        df = df.rename( columns = {'index': 'Date'} )\n",
    "    df = df.dropna(subset = ['Date'])\n",
    "    df['Date'] = pd.to_datetime( df['Date'])\n",
    "    del df['Difference']\n",
    "\n",
    "    return df\n",
    "\n",
    "def abnormal_df(symbol):\n",
    "    '''\n",
    "    df: Fundamental Info + Yahoo Daily Price + Sentiments\n",
    "    '''\n",
    "    # Processing Fundamental Info\n",
    "    print(symbol)\n",
    "    guru = 'GuruFocus/' + symbol + '_Guru.csv'\n",
    "    fundamental = pd.read_csv(guru, encoding = \"ISO-8859-1\")\n",
    "    fundamental['Filing Date'] = pd.to_datetime(fundamental['Filing Date'])\n",
    "\n",
    "    idx = pd.date_range('2011-01-01', '2017-07-15')\n",
    "    fundamental.set_index('Filing Date', inplace=True)\n",
    "    fundamental.index= pd.DatetimeIndex(fundamental.index)\n",
    "    fundamental = fundamental.reindex(idx, method='bfill')\n",
    "\n",
    "    # Sentiment Scores\n",
    "    sentiment = 'SentimentNews/Abnormal_' + symbol +'_News.csv'\n",
    "    news_df = pd.read_csv(sentiment, encoding = \"ISO-8859-1\")\n",
    "    news_df = news_df[news_df.Text != \"['nannan']\" ]\n",
    "    data= news_df.values\n",
    "    document = []\n",
    "    for row in data:\n",
    "        score = row[2].split(',')\n",
    "        if score[0][2:5] == 'pos':\n",
    "            document.append( [row[0], row[1], float(score[1][1:-1]) ])\n",
    "        elif score[0][2:5] == 'neg':\n",
    "            document.append( [row[0], row[1], float(score[1][1:-1])* -1.0 ])\n",
    "    news_df = pd.DataFrame(document, columns=['Date','Text','Score'])\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "    del news_df['Text']\n",
    "\n",
    "    # Stock Price df & Generate Labels\n",
    "    price = 'News/' + symbol+'_Stocks.csv'\n",
    "    price_df = pd.read_csv(price, encoding = \"ISO-8859-1\")\n",
    "    price_df['Return'] = price_df['Adj Close'].pct_change()\n",
    "\n",
    "    # Abnormal Return\n",
    "    abnormal_file = 'Abnormal_Returns/' + symbol + '_AbnormalReturn.csv'\n",
    "    abnormal_df = pd.read_csv(abnormal_file, encoding = \"ISO-8859-1\")\n",
    "    abnormal_df['Date'] = pd.to_datetime(abnormal_df['Date'])\n",
    "\n",
    "    # Stock Price + Fundamental Info + Sentiments\n",
    "    sentiment_price= news_df.set_index('Date').join(price_df.set_index('Date'))\n",
    "    sentiment_price_abnormal = sentiment_price.join(abnormal_df.set_index('Date'))\n",
    "    # df = sentiment_price_abnormal\n",
    "    df = sentiment_price_abnormal.join(fundamental)\n",
    "    df = df[pd.notnull(df.index)]\n",
    "    df = df.dropna()\n",
    "    del df['Fiscal Period']\n",
    "    del df['Restated Filing Date']\n",
    "    df = df.replace(to_replace = ['No Debt'], value = 0)\n",
    "\n",
    "    # df.fillna(0, inplace = True)\n",
    "    df['Target'] = np.nan\n",
    "    requirement = 0.00000\n",
    "    for i in range(len(df)):\n",
    "        if df['Abnormal Return'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Abnormal Return'].iloc[i] <  -requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    if 'index' in df.columns:\n",
    "        df = df.rename( columns = {'index': 'Date'} )\n",
    "    df = df.dropna(subset = ['Date'])\n",
    "    df['Date'] = pd.to_datetime( df['Date'])\n",
    "    del df['Abnormal Return']\n",
    "    return df  \n",
    "\n",
    "def df_no_sentiment(symbol):\n",
    "    '''\n",
    "    df: Fundamental Info + Yahoo Daily Price\n",
    "    '''\n",
    "    # Processing Fundamental Info\n",
    "    print(symbol)\n",
    "    guru = 'GuruFocus/' + symbol + '_Guru.csv'\n",
    "    fundamental = pd.read_csv(guru, encoding = \"ISO-8859-1\")\n",
    "    fundamental['Filing Date'] = pd.to_datetime(fundamental['Filing Date'])\n",
    "\n",
    "    idx = pd.date_range('2011-01-01', '2017-07-15')\n",
    "    fundamental.set_index('Filing Date', inplace=True)\n",
    "    fundamental.index= pd.DatetimeIndex(fundamental.index)\n",
    "    fundamental = fundamental.reindex(idx, method='bfill')\n",
    "\n",
    "    # Stock Price df & Generate Labels\n",
    "    price = 'News/' + symbol+'_Stocks.csv'\n",
    "    price_df = pd.read_csv(price, encoding = \"ISO-8859-1\")\n",
    "    price_df['Return'] = price_df['Adj Close'].pct_change()\n",
    "\n",
    "    sp500 = pd.read_csv('GuruFocus/Yahoo_Index_GSPC.csv')\n",
    "    sp500['Date'] = pd.to_datetime(sp500['Date'])\n",
    "    sp500['Sp_Return'] = sp500['Adj Close'].pct_change()\n",
    "    del sp500['Open']\n",
    "    del sp500['Close']\n",
    "    del sp500['High']\n",
    "    del sp500['Low']\n",
    "    del sp500['Volume']\n",
    "    del sp500['Adj Close']\n",
    "\n",
    "    price_df= price_df.set_index('Date').join(sp500.set_index('Date'))\n",
    "    price_df = price_df.dropna()\n",
    "    price_df['Difference'] = price_df['Return'] - price_df['Sp_Return']\n",
    "\n",
    "    # Stock Price + Fundamental Info\n",
    "    df = price_df.join(fundamental)\n",
    "    df = df[pd.notnull(df.index)]\n",
    "    df = df.dropna()\n",
    "    del df['Fiscal Period']\n",
    "    del df['Restated Filing Date']\n",
    "    df = df.replace(to_replace = ['No Debt'], value = 0)\n",
    "\n",
    "    df['Target'] = np.nan\n",
    "    requirement = 0.0\n",
    "    for i in range(len(df)):\n",
    "        if df['Difference'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Difference'].iloc[i] <  requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "    del df['Sp_Return']\n",
    "    # del df['Difference']\n",
    "\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    if 'index' in df.columns:\n",
    "        df = df.rename( columns = {'index': 'Date'} )\n",
    "    df = df.dropna(subset = ['Date'])\n",
    "    df['Date'] = pd.to_datetime( df['Date'])\n",
    "    del df['Difference']\n",
    "\n",
    "    return df\n",
    "\n",
    "def df_no_fundamental(symbol):\n",
    "    '''\n",
    "    df: Yahoo Daily Price\n",
    "    '''\n",
    "    # Stock Price df & Generate Labels\n",
    "    print(symbol)\n",
    "    price = 'News/' + symbol+'_Stocks.csv'\n",
    "    price_df = pd.read_csv(price, encoding = \"ISO-8859-1\")\n",
    "    price_df['Return'] = price_df['Adj Close'].pct_change()\n",
    "\n",
    "    sp500 = pd.read_csv('GuruFocus/Yahoo_Index_GSPC.csv')\n",
    "    sp500['Date'] = pd.to_datetime(sp500['Date'])\n",
    "    sp500['Sp_Return'] = sp500['Adj Close'].pct_change()\n",
    "    del sp500['Open']\n",
    "    del sp500['Close']\n",
    "    del sp500['High']\n",
    "    del sp500['Low']\n",
    "    del sp500['Volume']\n",
    "    del sp500['Adj Close']\n",
    "\n",
    "    price_df= price_df.set_index('Date').join(sp500.set_index('Date'))\n",
    "    price_df = price_df.dropna()\n",
    "    price_df['Difference'] = price_df['Return'] - price_df['Sp_Return']\n",
    "\n",
    "    df = price_df \n",
    "\n",
    "    df['Target'] = np.nan\n",
    "    requirement = 0.0\n",
    "    for i in range(len(df)):\n",
    "        if df['Difference'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Difference'].iloc[i] <  requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "\n",
    "    del df['Sp_Return']\n",
    "    del df['Difference']\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    if 'index' in df.columns:\n",
    "        df = df.rename( columns = {'index': 'Date'} )\n",
    "    df = df.dropna(subset = ['Date'])\n",
    "    df['Date'] = pd.to_datetime( df['Date'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def combine_multiple_companies(company_list):\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    multiple_dfs = []\n",
    "\n",
    "    for company in company_list:\n",
    "        df = combine_final_df(company)\n",
    "        multiple_dfs.append(df)\n",
    "\n",
    "    # total = pd.DataFrame()\n",
    "    # for company in company_list:\n",
    "    #     total = total.append(df_no_sentiment(company), ignore_index)\n",
    "\n",
    "    total = pd.concat(multiple_dfs, ignore_index = True)\n",
    "    total = total.set_index('Date')\n",
    "    total = total.interpolate(method = 'time')\n",
    "    total = total.fillna(total.mean())\n",
    "    total =  shuffle(total)\n",
    "    return total\n",
    "\n",
    "def prepareDataForClassification(df, test_size): \n",
    "    X = np.array(df.drop(['Target'], 1))\n",
    "    X = preprocessing.scale(X)\n",
    "    y = np.array(df['Target'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= test_size)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def PCA_prepareDataForClassification(df, n_component, test_size):\n",
    "    X = np.array(df.drop(['Target'], 1))\n",
    "    X = preprocessing.scale(X)\n",
    "    y = np.array(df['Target'])\n",
    "    \n",
    "    pca = FastICA(n_components = n_component).fit(X)\n",
    "    transformed_X = pca.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(transformed_X,y, test_size= test_size)\n",
    "    \n",
    "    # var = pca.explained_variance_ratio_\n",
    "    # print(sum(var))\n",
    "    # n = list( range(1, n_component+1) )\n",
    "    # plt.xlabel('Principal Components')\n",
    "    # plt.ylabel('Percent of Variance Explained')\n",
    "    # plt.bar(n, var)\n",
    "    # plt.show()\n",
    "    return X_train, X_test, y_train, y_test\n",
    "   \n",
    "\n",
    "def performTimeSeriesSearchGrid(X_train, y_train, folds, method, grid):\n",
    "    \"\"\"\n",
    "    parameters is a dictionary with: keys --> parameter , values --> list of values of parameter\n",
    "    \"\"\"\n",
    "    print('')\n",
    "    print('Performing Search Grid CV...')\n",
    "    print('Algorithm: ', method)\n",
    "    param = grid.keys()\n",
    "    finalGrid = {}\n",
    "    if len(param) == 1:\n",
    "        for value_0 in grid[param[0]]:\n",
    "            parameters = [value_0]\n",
    "            accuracy = performCV(dataset, folds, split, features, method, parameters)\n",
    "            finalGrid[accuracy] = parameters\n",
    "        final = sorted(finalGrid.iteritems(), key=operator.itemgetter(0), reverse=True)  \n",
    "        print('')\n",
    "        print('Final CV Results: ', final)      \n",
    "        return final[0]\n",
    "        \n",
    "    elif len(param) == 2:\n",
    "        for value_0 in grid[param[0]]:\n",
    "            for value_1 in grid[param[1]]:\n",
    "                parameters = [value_0, value_1]\n",
    "                accuracy = performCV(dataset, folds, split, features, method, parameters)\n",
    "                finalGrid[accuracy] = parameters\n",
    "        final = sorted(finalGrid.iteritems(), key=operator.itemgetter(0), reverse=True)\n",
    "        print('')\n",
    "        print('Final CV Results: ', final)\n",
    "        return final[0]\n",
    "\n",
    "###############################################################################\n",
    "######## CLASSIFICATION    \n",
    "      \n",
    "\n",
    "def prepareDataForModelSelection(X_train, y_train, start_validation):\n",
    "    \"\"\"\n",
    "    gets train set and generates a validation set splitting the train.\n",
    "    The validation set is mandatory for feature and model selection.\n",
    "    \"\"\"\n",
    "    X = X_train[X_train.index < start_validation]\n",
    "    y = y_train[y_train.index < start_validation]    \n",
    "    \n",
    "    X_val = X_train[X_train.index >= start_validation]    \n",
    "    y_val = y_train[y_train.index >= start_validation]   \n",
    "    \n",
    "    return X, y, X_val, y_val\n",
    "  \n",
    "def performClassification(X_train, y_train, X_test, y_test, method):\n",
    "    \"\"\"\n",
    "    performs classification on returns using serveral algorithms\n",
    "    \"\"\"\n",
    "    #print ''\n",
    "    print('Performing ' + method + ' Classification...')   \n",
    "    print('Size of train set: ', X_train.shape)\n",
    "    print('Size of test set: ', X_test.shape)\n",
    "   \n",
    "    if method == 'RF':   \n",
    "        return performRFClass(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    elif method == 'KNN':\n",
    "        return performKNNClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif method == 'SVM':   \n",
    "        return performSVMClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # elif method == 'ADA':\n",
    "    #     return performAdaBoostClass(X_train, y_train, X_test, y_test, parameters)\n",
    "    \n",
    "    elif method == 'GTB': \n",
    "        return performGTBClass(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    elif method == 'QuadraticDiscriminantAnalysis': \n",
    "        return performQDAClass(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    elif method == 'MLP': \n",
    "        return performMLPClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "def performRFClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Binary Classification\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    " \n",
    "    y_pred_train = clf.predict(X_train)  \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "   \n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ' ,test_mse )\n",
    "    print('train time: ', train_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "        \n",
    "def performKNNClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    KNN binary Classification\n",
    "    \"\"\"\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ' , test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "def performSVMClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    SVM binary Classification\n",
    "    \"\"\"\n",
    "    clf = SVC()\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ', test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "    \n",
    "def performAdaBoostClass(X_train, y_train, X_test, y_test, parameters):\n",
    "    \"\"\"\n",
    "    Ada Boosting binary Classification\n",
    "    \"\"\"\n",
    "    n = parameters[0]\n",
    "    l =  parameters[1]\n",
    "    clf = AdaBoostClassifier(n_estimators = n, learning_rate = l)\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ' ,test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "\n",
    "    \n",
    "def performGTBClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Tree Boosting binary Classification\n",
    "    \"\"\"\n",
    "    clf = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ' ,test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "def performQDAClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Tree Boosting binary Classification\n",
    "    \"\"\"\n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ', test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "def performMLPClass(X_train, y_train, X_test, y_test):\n",
    "    clf = MLPClassifier()\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    fit_time = end-start\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end-start\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print('accuracy: ', accuracy,'train mse: ' ,train_mse, 'test mse: ' ,test_mse )\n",
    "    print('fit time: ', fit_time, 'train time: ', train_time, 'test time: ', test_time)\n",
    "    return accuracy, train_mse, test_mse\n",
    "\n",
    "##############################################################################\n",
    "####### REGRESSION\n",
    "    \n",
    "def performRegression(dataset, split):\n",
    "    \"\"\"\n",
    "    performs regression on returns using serveral algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    features = dataset.columns[1:]\n",
    "    index = int(np.floor(dataset.shape[0]*split))\n",
    "    train, test = dataset[:index], dataset[index:]\n",
    "    print('Size of train set: ', train.shape)\n",
    "    print('Size of test set: ', test.shape)\n",
    "    \n",
    "    output = 'Return_SP500'\n",
    "\n",
    "    print('Accuracy RFC: ', performRFReg(train, test, features, output))\n",
    "   \n",
    "    print('Accuracy SVM: ', performSVMReg(train, test, features, output))\n",
    "   \n",
    "    print('Accuracy BAG: ', performBaggingReg(train, test, features, output))\n",
    "   \n",
    "    print('Accuracy ADA: ', performAdaBoostReg(train, test, features, output))\n",
    "   \n",
    "    print('Accuracy BOO: ', performGradBoostReg(train, test, features, output))\n",
    "\n",
    "    print('Accuracy KNN: ', performKNNReg(train, test, features, output))\n",
    "\n",
    "\n",
    "def performRFReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Random Forest Regression\n",
    "    \"\"\"\n",
    "\n",
    "    forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    forest = forest.fit(train[features], train[output])\n",
    "    Predicted = forest.predict(test[features])\n",
    "    \n",
    "\n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output], Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performSVMReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    SVM Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = SVR()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "    \n",
    "def performBaggingReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Bagging Regression\n",
    "    \"\"\"\n",
    "  \n",
    "    clf = BaggingRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)  \n",
    "\n",
    "def performAdaBoostReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Ada Boost Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = AdaBoostRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performGradBoostReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regression\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = GradientBoostingRegressor()\n",
    "    clf.fit(test[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()    \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performKNNReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    KNN Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = KNeighborsRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def cal_pos_neg_label(df):\n",
    "    print( \"Total number of Instances: \", len(df) )\n",
    "\n",
    "    pos = df[df['Target'] == 1.0] \n",
    "    print( \"Total number of Positive Labels: \", len(pos) ) # 629 instances\n",
    "\n",
    "    neg = df[df['Target'] == -1.0]\n",
    "    print( \"Total number of Positive Labels: \", len(neg) ) # 624 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005930.KS\n",
      "AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTC\n",
      "MSFT\n",
      "ORCL\n",
      "SNE\n",
      "TSLA\n",
      "TDC\n",
      "TXN\n",
      "FB\n",
      "AMZN\n",
      "QCOM\n",
      "GOOG.O\n",
      "IBM\n",
      "CVX\n",
      "GE\n",
      "WMT\n",
      "WFC\n",
      "XOM\n",
      "T\n",
      "F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:303: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Instances:  13579\n",
      "Total number of Positive Labels:  6668\n",
      "Total number of Positive Labels:  6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing MLP Classification...\n",
      "Size of train set:  (8147, 150)\n",
      "Size of test set:  (5432, 150)\n",
      "accuracy:  0.7934462444771723 train mse:  0.7143733889775378 test mse:  0.8262150220913107\n",
      "fit time:  10.472677946090698 train time:  0.012392759323120117 test time:  0.009651899337768555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "target = 'CLASSIFICATION'\n",
    "#target = 'REGRESSION'\n",
    "\n",
    "\n",
    "#  test on multiple company\n",
    "company_list = ['005930.KS','AAPL', 'INTC', 'MSFT', 'ORCL', 'SNE',\n",
    "                 'TSLA','TDC', 'TXN','FB', 'AMZN', 'QCOM', 'GOOG.O',\n",
    "                'IBM', 'CVX', 'GE','WMT', 'WFC', 'XOM','T','F']\n",
    "\n",
    "\n",
    "datasets = combine_multiple_companies(company_list)\n",
    "cal_pos_neg_label(datasets)\n",
    "\n",
    "# save_data = open(\"pickled_algos/Abnormal_Guru_Yahoo_data.pickle\",\"wb\")\n",
    "# pickle.dump(datasets, save_data)\n",
    "# save_data.close()\n",
    "\n",
    "# open_data_f = open(\"pickled_algos/Abnormal_Guru_Yahoo_data.pickle\", \"rb\")\n",
    "# datasets = pickle.load(open_data_f)\n",
    "# open_data_f.close()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = prepareDataForClassification(datasets, 0.4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = PCA_prepareDataForClassification(datasets,150, 0.4)\n",
    "\n",
    "if target == 'CLASSIFICATION':\n",
    "    performClassification(X_train, y_train, X_test, y_test, 'MLP')\n",
    "    print('')\n",
    "\n",
    "elif target == 'REGRESSION':\n",
    "    performRegression(datasets, 0.8)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
